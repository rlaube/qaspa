{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9727b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def check_path(path):\n",
    "    d = os.path.dirname(path)\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "merged_relations = [\n",
    "    'antonym',\n",
    "    'atlocation',\n",
    "    'capableof',\n",
    "    'causes',\n",
    "    'createdby',\n",
    "    'isa',\n",
    "    'desires',\n",
    "    'hassubevent',\n",
    "    'partof',\n",
    "    'hascontext',\n",
    "    'hasproperty',\n",
    "    'madeof',\n",
    "    'notcapableof',\n",
    "    'notdesires',\n",
    "    'receivesaction',\n",
    "    'relatedto',\n",
    "    'usedfor',\n",
    "]\n",
    "\n",
    "spaced_merged_relations = [\n",
    "    'antonym',\n",
    "    'at location',\n",
    "    'capable of',\n",
    "    'causes',\n",
    "    'created by',\n",
    "    'is a',\n",
    "    'desires',\n",
    "    'has sub-event',\n",
    "    'part of',\n",
    "    'has context',\n",
    "    'has property',\n",
    "    'made of',\n",
    "    'not capable of',\n",
    "    'not desires',\n",
    "    'receives action',\n",
    "    'related to',\n",
    "    'used for',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf28ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# MODEL_CLASS_TO_NAME = {\n",
    "#     'gpt': list(OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()),\n",
    "#     'bert': list(BERT_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()),\n",
    "#     'xlnet': list(XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()),\n",
    "#     'roberta': list(ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()),\n",
    "#     'lstm': ['lstm'],\n",
    "# }\n",
    "model_class = AutoModel\n",
    "tokenizer_class = AutoTokenizer\n",
    "model_name = 'bert-large-uncased'\n",
    "# model_name = 'roberta-large'\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "model = model_class.from_pretrained(model_name, output_hidden_states=True)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43092d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2487810/2487810 [03:06<00:00, 13364.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs dumped\n"
     ]
    }
   ],
   "source": [
    "prune = False\n",
    "cpnet_csv_path = '../data/cpnet/conceptnet.en.csv'\n",
    "cpnet_vocab_path = '../data/cpnet/concept.txt'\n",
    "blacklist = set([\"uk\", \"us\", \"take\", \"make\", \"object\", \"person\", \"people\"])  # issue: mismatch with the blacklist in grouding.py\n",
    "\n",
    "concept2id = {}\n",
    "id2concept = {}\n",
    "with open(cpnet_vocab_path, \"r\", encoding=\"utf8\") as fin:\n",
    "    id2concept = [w.strip() for w in fin]\n",
    "concept2id = {w: i for i, w in enumerate(id2concept)}\n",
    "\n",
    "id2relation = merged_relations\n",
    "relation2id = {r: i for i, r in enumerate(id2relation)}\n",
    "\n",
    "max_seq_length = 128\n",
    "# BERT inputs\n",
    "all_input_ids, all_input_mask, all_segment_ids, all_sub_span, all_rel_span, all_obj_span= [], [], [], [], [], []\n",
    "\n",
    "# keep track of input entity ids\n",
    "nrow = sum(1 for _ in open(cpnet_csv_path, 'r', encoding='utf-8'))\n",
    "entity_ids = []  # stores [sub, rel, obj] for each triple\n",
    "\n",
    "with open(cpnet_csv_path, \"r\", encoding=\"utf8\") as fin:\n",
    "\n",
    "    def not_save(cpt):\n",
    "        if cpt in blacklist:\n",
    "            return True\n",
    "        '''originally phrases like \"branch out\" would not be kept in the graph'''\n",
    "        # for t in cpt.split(\"_\"):\n",
    "        #     if t in nltk_stopwords:\n",
    "        #         return True\n",
    "        return False\n",
    "    \n",
    "    attrs = set()\n",
    "    \n",
    "    for line in tqdm(fin, total=nrow):\n",
    "        ls = line.strip().split('\\t')\n",
    "        rel = relation2id[ls[0]]\n",
    "        subj = concept2id[ls[1]]\n",
    "        obj = concept2id[ls[2]]\n",
    "\n",
    "        if prune and (not_save(ls[1]) or not_save(ls[2]) or id2relation[rel] == \"hascontext\"):\n",
    "            continue\n",
    "        \n",
    "        if subj == obj:  # delete loops\n",
    "            continue\n",
    "\n",
    "        if (subj, obj, rel) not in attrs:\n",
    "            attrs.add((subj, obj, rel))\n",
    "            entity_ids.append([subj, rel, obj])\n",
    "            # tokenize inputs and format input data for BERT\n",
    "            sub_tokens = tokenizer.tokenize(ls[1].replace('_', ' '))\n",
    "            rel_tokens = tokenizer.tokenize(spaced_merged_relations[rel])\n",
    "            obj_tokens = tokenizer.tokenize(ls[2].replace('_', ' '))\n",
    "            triple_tokens = [tokenizer.cls_token] + sub_tokens + rel_tokens + obj_tokens + [tokenizer.sep_token]\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(triple_tokens)\n",
    "\n",
    "            assert len(input_ids) <= max_seq_length\n",
    "            pad_len = max_seq_length - len(input_ids)\n",
    "            input_mask = [1] * len(input_ids) + [0] * pad_len\n",
    "            input_ids += [0] * pad_len\n",
    "            segment_ids = [0] * max_seq_length # all just one sentence (no sentence pair)\n",
    "            # define span of sub, rel, and obj\n",
    "            sub_span = [1, len(sub_tokens)]  \n",
    "            rel_span = [sub_span[-1] + 1, sub_span[-1] + len(rel_tokens)]\n",
    "            obj_span = [rel_span[-1] + 1, rel_span[-1] + len(obj_tokens)]\n",
    "            \n",
    "            all_input_ids.append(input_ids)\n",
    "            all_input_mask.append(input_mask)\n",
    "            all_segment_ids.append(segment_ids)\n",
    "            all_sub_span.append(sub_span)\n",
    "            all_rel_span.append(rel_span)\n",
    "            all_obj_span.append(obj_span)\n",
    "            \n",
    "        \n",
    "entity_ids = np.array(entity_ids)\n",
    "\n",
    "cache_path = '../data/cpnet/encoder_inputs/'+model_name+'.pkl'\n",
    "check_path(cache_path)\n",
    "\n",
    "with open(cache_path, 'wb') as fout:\n",
    "    pickle.dump((all_input_ids, all_input_mask, all_segment_ids, all_sub_span, all_rel_span, all_obj_span, entity_ids), fout)\n",
    "print('Inputs dumped')\n",
    "\n",
    "# return BERT encodings of subj, obj, rel using tokens\n",
    "# add embeddings to each entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = '../data/cpnet/encoder_inputs/'+model_name+'.pkl'\n",
    "with open(cache_path, 'rb') as fin:\n",
    "    all_input_ids, all_input_mask, all_segment_ids, all_sub_span, all_rel_span, all_obj_span, entity_ids = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aab931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 9228/9228 [2:07:37<00:00,  1.21it/s]  \n"
     ]
    }
   ],
   "source": [
    "cpnet_csv_path = '../data/cpnet/conceptnet.en.csv'\n",
    "cpnet_vocab_path = '../data/cpnet/concept.txt'\n",
    "\n",
    "concept2id = {}\n",
    "id2concept = {}\n",
    "with open(cpnet_vocab_path, \"r\", encoding=\"utf8\") as fin:\n",
    "    id2concept = [w.strip() for w in fin]\n",
    "concept2id = {w: i for i, w in enumerate(id2concept)}\n",
    "\n",
    "id2relation = merged_relations\n",
    "relation2id = {r: i for i, r in enumerate(id2relation)}\n",
    "\n",
    "all_input_ids, all_input_mask, all_segment_ids, all_sub_span, all_rel_span, all_obj_span = [torch.tensor(x, dtype=torch.long) for x in [all_input_ids, all_input_mask, all_segment_ids, all_sub_span, all_rel_span, all_obj_span]]\n",
    "\n",
    "n = entity_ids.shape[0]\n",
    "batch_size = 256\n",
    "\n",
    "assert n == all_input_ids.shape[0]\n",
    "\n",
    "# which layer of BERT to use for embeddings\n",
    "layer = -1\n",
    "emb_dim = 1024\n",
    "max_seq_length = 128\n",
    "cpnet_concept_emb = torch.zeros((len(concept2id), emb_dim)).to(device)\n",
    "cpnet_rel_emb = torch.zeros((len(relation2id), emb_dim)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mask = torch.arange(max_seq_length, device=device)[None, :]\n",
    "\n",
    "    for a in tqdm(range(0, n, batch_size), total=n // batch_size + 1, desc='Extracting features'):\n",
    "        b = min(a + batch_size, n)\n",
    "        *batch, sub_span, rel_span, obj_span = [x.to(device) for x in [all_input_ids[a:b], all_input_mask[a:b], all_segment_ids[a:b], all_sub_span[a:b], all_rel_span[a:b], all_obj_span[a:b]]]\n",
    "        outputs = model(*batch)\n",
    "        \n",
    "        hidden_states = outputs[-1][layer]\n",
    "        \n",
    "        sub_mask = (mask >= sub_span[:, 0, None]) & (mask <= sub_span[:, 1, None])\n",
    "        rel_mask = (mask >= rel_span[:, 0, None]) & (mask <= rel_span[:, 1, None])\n",
    "        obj_mask = (mask >= obj_span[:, 0, None]) & (mask <= obj_span[:, 1, None])\n",
    "        # apply mask using the spans, and average the token rep by dividing by span length\n",
    "        sub_pooled = (hidden_states * sub_mask.float().unsqueeze(-1)).sum(1) / (sub_span[:,1].float() - sub_span[:,0].float() + 1).unsqueeze(1)\n",
    "        rel_pooled = (hidden_states * rel_mask.float().unsqueeze(-1)).sum(1) / (rel_span[:,1].float() - rel_span[:,0].float() + 1).unsqueeze(1)\n",
    "        obj_pooled = (hidden_states * obj_mask.float().unsqueeze(-1)).sum(1) / (obj_span[:,1].float() - obj_span[:,0].float() + 1).unsqueeze(1)\n",
    "        \n",
    "        sub_ids = entity_ids[a:b, 0]\n",
    "        rel_ids = entity_ids[a:b, 1]\n",
    "        obj_ids = entity_ids[a:b, 2]\n",
    "        for i, (sub_id, rel_id, obj_id) in enumerate(zip(sub_ids, rel_ids, obj_ids)):\n",
    "            cpnet_concept_emb[sub_id] += sub_pooled[i]\n",
    "            cpnet_concept_emb[obj_id] += obj_pooled[i]\n",
    "            cpnet_rel_emb[rel_id] += rel_pooled[i]\n",
    "\n",
    "sub_unique, sub_counts = np.unique(entity_ids[:,0], return_counts=True) \n",
    "rel_unique, rel_counts_unordered = np.unique(entity_ids[:,1], return_counts=True) \n",
    "obj_unique, obj_counts = np.unique(entity_ids[:,2], return_counts=True) \n",
    "rel_counts = np.zeros(len(merged_relations))\n",
    "rel_counts[rel_unique] += rel_counts_unordered\n",
    "concept_counts = np.zeros(len(concept2id))\n",
    "concept_counts[sub_unique] += sub_counts\n",
    "concept_counts[obj_unique] += obj_counts \n",
    "\n",
    "cpnet_concept_emb = cpnet_concept_emb.to('cpu').numpy()\n",
    "cpnet_rel_emb = cpnet_rel_emb.to('cpu').numpy()\n",
    "cpnet_concept_emb = np.divide(cpnet_concept_emb, concept_counts[:,np.newaxis], out=np.zeros_like(cpnet_concept_emb), where=concept_counts[:,np.newaxis]!=0)\n",
    "cpnet_rel_emb = cpnet_rel_emb / rel_counts[:,np.newaxis]\n",
    "output_dir = '../data/cpnet/encoder_embs/'\n",
    "check_path(output_dir)\n",
    "np.save(output_dir + model_name + '_concept_emb', cpnet_concept_emb)\n",
    "np.save(output_dir + model_name + '_rel_emb', cpnet_rel_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qaspa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
