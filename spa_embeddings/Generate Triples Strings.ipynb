{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE FOR EMBEDDING ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMLS for MEDQA\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# load concept and relations vocabs \n",
    "\n",
    "umls_vocab_id_path = '../data/umls/concepts.txt'\n",
    "umls_vocab_path = '../data/umls/concept_names.txt'\n",
    "umls_rel_path = '../data/umls/relations.txt'\n",
    "\n",
    "concept2id = {}\n",
    "id2concept = []\n",
    "with open(umls_vocab_path, \"r\", encoding=\"utf8\") as fin:\n",
    "    # id2concept = {w.strip().split('\\t')[0]: w.strip().split('\\t')[1].replace(' ', '_').lower() for w in fin}\n",
    "    id2concept = [w.strip().split('\\t')[1].replace(' ', '_').lower() for w in fin]\n",
    "concept2id = {w: i for i, w in enumerate(id2concept)}\n",
    "\n",
    "umls_vocab_id_path = '../data/umls/concepts.txt'\n",
    "with open(umls_vocab_id_path, \"r\", encoding=\"utf8\") as fin:\n",
    "    idx2id = [w.strip() for w in fin]\n",
    "\n",
    "id2relation = [rel.strip() for rel in open(umls_rel_path)]\n",
    "relation2id = {r: i for i, r in enumerate(id2relation)}\n",
    "spaced_relations = [rel.replace('_', ' ') for rel in id2relation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMLS for MEDQA\n",
    "\n",
    "import csv\n",
    "\n",
    "# load graph adjacency and concept id info\n",
    "dataset = '../data/medqa'\n",
    "split = 'test'\n",
    "graph_file = split+'_graph_ids_pruned'+'.npz'\n",
    "graphs = np.load(dataset+'/'+graph_file, allow_pickle=True)\n",
    "dataset_size = len(graphs['concept_ids'])\n",
    "\n",
    "concept_ids = graphs['concept_ids']\n",
    "rels = graphs['rels']\n",
    "subjects = graphs['subjects']\n",
    "objects = graphs['objects']\n",
    "\n",
    "all_triples_strings = []\n",
    "for i in range(dataset_size):\n",
    "    triples_strings = []\n",
    "    for subj, rel, obj in zip(subjects[i], rels[i], objects[i]):\n",
    "        subj_id = concept_ids[i][subj]\n",
    "        obj_id = concept_ids[i][obj]\n",
    "        subj_str = id2concept[idx2id[subj_id]]\n",
    "        rel_str = id2relation[rel]\n",
    "        obj_str = id2concept[idx2id[obj_id]]\n",
    "        \n",
    "        triples_strings.append(\" \".join([subj_str, rel_str, obj_str]))\n",
    "    all_triples_strings.append(triples_strings)\n",
    "\n",
    "with open(dataset+'/'+split+'_pruned_triples_strings.csv', \"w\", encoding='utf-8') as f:\n",
    "        wr = csv.writer(f)\n",
    "        wr.writerows(all_triples_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbe0cb0c05d434aa7b12da982542c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # CPNet for CSQA and OBQA\n",
    "\n",
    "# import csv\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# cpnet_vocab_path = '../data/cpnet/concept.txt'\n",
    "# merged_relations = [\n",
    "#     'antonym',\n",
    "#     'atlocation',\n",
    "#     'capableof',\n",
    "#     'causes',\n",
    "#     'createdby',\n",
    "#     'isa',\n",
    "#     'desires',\n",
    "#     'hassubevent',\n",
    "#     'partof',\n",
    "#     'hascontext',\n",
    "#     'hasproperty',\n",
    "#     'madeof',\n",
    "#     'notcapableof',\n",
    "#     'notdesires',\n",
    "#     'receivesaction',\n",
    "#     'relatedto',\n",
    "#     'usedfor',\n",
    "# ]\n",
    "\n",
    "# concept2id = {}\n",
    "# id2concept = {}\n",
    "# with open(cpnet_vocab_path, \"r\", encoding=\"utf8\") as fin:\n",
    "#     id2concept = [w.strip().upper() for w in fin]\n",
    "# concept2id = {w: i for i, w in enumerate(id2concept)}\n",
    "\n",
    "# id2relation = [rel.upper() for rel in merged_relations]\n",
    "# relation2id = {r: i for i, r in enumerate(id2relation)}\n",
    "\n",
    "# # load graph adjacency and concept id info\n",
    "# dataset = '../data/csqa'\n",
    "# split = 'train'\n",
    "# graph_file = split+'_graph_ids_pruned'+'.npz'\n",
    "# graphs = np.load(dataset+'/'+graph_file, allow_pickle=True)\n",
    "# dataset_size = len(graphs['concept_ids'])\n",
    "\n",
    "# concept_ids = graphs['concept_ids']\n",
    "# rels = graphs['rels']\n",
    "# subjects = graphs['subjects']\n",
    "# objects = graphs['objects']\n",
    "\n",
    "# all_triples_strings = []\n",
    "# for i in tqdm(range(dataset_size)):\n",
    "#     triples_strings = []\n",
    "#     for subj, rel, obj in zip(subjects[i], rels[i], objects[i]):\n",
    "#         subj_id = concept_ids[i][subj]\n",
    "#         obj_id = concept_ids[i][obj]\n",
    "#         subj_str = id2concept[subj_id]\n",
    "#         rel_str = id2relation[rel]\n",
    "#         obj_str = id2concept[obj_id]\n",
    "        \n",
    "#         triples_strings.append(\" \".join([subj_str, rel_str, obj_str]))\n",
    "#     all_triples_strings.append(triples_strings)\n",
    "\n",
    "# with open(dataset+'/'+split+'_pruned_triples_strings.csv', \"w\", encoding='utf-8') as f:\n",
    "#         wr = csv.writer(f)\n",
    "#         wr.writerows(all_triples_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random tests\n",
    "\n",
    "# import nengo_spa as spa\n",
    "# alg = spa.algebras.HrrAlgebra()\n",
    "# dim=1024\n",
    "# concept_sp = np.load('../cpnet/encoder_embs/bert-large-uncased_concept_emb_v2.npy')\n",
    "# rel_sp = np.load('../cpnet/encoder_embs/bert-large-uncased_rel_emb_v2.npy')\n",
    "# for i, concept in enumerate(concept_sp):\n",
    "#     if np.linalg.norm(concept) > 0:\n",
    "#         concept_sp[i] = alg.make_unitary(concept)\n",
    "# for i, rel in enumerate(rel_sp):\n",
    "#     if np.linalg.norm(rel) > 0:\n",
    "#         rel_sp[i] = alg.make_unitary(rel)\n",
    "\n",
    "# i = 0\n",
    "# graph_sp = np.zeros(dim)\n",
    "# for subj, rel, obj in zip(subjects[i], rels[i], objects[i]):\n",
    "#     subj_id = concept_ids[i][subj]\n",
    "#     obj_id = concept_ids[i][obj]\n",
    "#     # subj_str = vocab_dict['CONCEPT'+str(subj_id)]\n",
    "#     # rel_str = vocab_dict['RELATION'+str(rel)]\n",
    "#     # obj_str = vocab_dict['CONCEPT'+str(obj_id)]\n",
    "    \n",
    "#     # do (subj * rel) * obj\n",
    "#     assert(np.linalg.norm(concept_sp[subj_id]) > 0)\n",
    "#     assert(np.linalg.norm(concept_sp[obj_id]) > 0)\n",
    "#     triple = alg.bind(\n",
    "#                     alg.bind(concept_sp[subj_id], rel_sp[rel]), \n",
    "#                     concept_sp[obj_id])\n",
    "#     graph_sp += triple"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qaspa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
