{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9727b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "# from transformers import (OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP, BERT_PRETRAINED_CONFIG_ARCHIVE_MAP,\n",
    "#                           XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP, ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP)\n",
    "# from transformers import BertModel, BertTokenizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def check_path(path):\n",
    "    d = os.path.dirname(path)\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf28ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_CLASS_TO_NAME = {\n",
    "#     'gpt': list(OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()),\n",
    "#     'bert': list(BERT_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()),\n",
    "#     'xlnet': list(XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()),\n",
    "#     'roberta': list(ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP.keys()),\n",
    "#     'lstm': ['lstm'],\n",
    "# }\n",
    "model_class = AutoModel\n",
    "tokenizer_class = AutoTokenizer\n",
    "model_name = 'michiyasunaga/BioLinkBERT-large'\n",
    "tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "model = model_class.from_pretrained(model_name, output_hidden_states=True)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9617c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '../data/umls/encoder_inputs/'\n",
    "outfile = 'BioLinkBERT-inputs'\n",
    "\n",
    "umls_csv_path = './umls.csv'\n",
    "umls_vocab_path = './concept_names.txt'\n",
    "umls_rel_path = './relations.txt'\n",
    "\n",
    "concept2id = {}\n",
    "id2concept = {}\n",
    "with open(umls_vocab_path, \"r\", encoding=\"utf8\") as fin:\n",
    "    id2concept = {w.strip().split('\\t')[0]: w.strip().split('\\t')[1] for w in fin}\n",
    "concept2id = {w: i for i, w in enumerate(id2concept)}\n",
    "\n",
    "id2relation = [rel.strip() for rel in open(umls_rel_path)]\n",
    "relation2id = {r: i for i, r in enumerate(id2relation)}\n",
    "spaced_relations = [rel.replace('_', ' ') for rel in id2relation]\n",
    "\n",
    "max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43092d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1212586/1212586 [02:14<00:00, 9044.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs dumped\n"
     ]
    }
   ],
   "source": [
    "# BERT inputs\n",
    "all_input_ids, all_input_mask, all_segment_ids, all_sub_span, all_rel_span, all_obj_span= [], [], [], [], [], []\n",
    "\n",
    "loops = 0\n",
    "repeats = 0\n",
    "# keep track of input entity ids\n",
    "nrow = sum(1 for _ in open(umls_csv_path, 'r', encoding='utf-8'))\n",
    "entity_ids = []  # stores [sub, rel, obj] for each triple\n",
    "\n",
    "with open(umls_csv_path, \"r\", encoding=\"utf8\") as fin:\n",
    "    \n",
    "    attrs = set()\n",
    "    \n",
    "    for line in tqdm(fin, total=nrow):\n",
    "        ls = line.strip().split('\\t')\n",
    "        rel = relation2id[ls[0]] if not(ls[0] == 'isa') else relation2id['is_a']  \n",
    "        subj = ls[1]\n",
    "        obj = ls[2]\n",
    "        \n",
    "        if subj == obj:  # delete loops\n",
    "            loops = loops + 1\n",
    "            continue\n",
    "\n",
    "        if (subj, obj, rel) not in attrs:\n",
    "            attrs.add((subj, obj, rel))\n",
    "            entity_ids.append([subj, rel, obj])\n",
    "            # tokenize inputs and format input data for BERT\n",
    "            sub_tokens = tokenizer.tokenize(id2concept[subj])\n",
    "            rel_tokens = tokenizer.tokenize(id2relation[rel].replace('_', ' '))\n",
    "            obj_tokens = tokenizer.tokenize(id2concept[obj])\n",
    "            triple_tokens = [tokenizer.cls_token] + sub_tokens + rel_tokens + obj_tokens + [tokenizer.sep_token]\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(triple_tokens)\n",
    "\n",
    "            assert len(input_ids) <= max_seq_length\n",
    "            pad_len = max_seq_length - len(input_ids)\n",
    "            input_mask = [1] * len(input_ids) + [0] * pad_len\n",
    "            input_ids += [0] * pad_len\n",
    "            segment_ids = [0] * max_seq_length # all just one sentence (no sentence pair)\n",
    "            # define span of sub, rel, and obj\n",
    "            sub_span = [1, len(sub_tokens)]  \n",
    "            rel_span = [sub_span[-1] + 1, sub_span[-1] + len(rel_tokens)]\n",
    "            obj_span = [rel_span[-1] + 1, rel_span[-1] + len(obj_tokens)]\n",
    "            \n",
    "            all_input_ids.append(input_ids)\n",
    "            all_input_mask.append(input_mask)\n",
    "            all_segment_ids.append(segment_ids)\n",
    "            all_sub_span.append(sub_span)\n",
    "            all_rel_span.append(rel_span)\n",
    "            all_obj_span.append(obj_span)\n",
    "        else:\n",
    "            repeats = repeats + 1\n",
    "        \n",
    "entity_ids = np.array(entity_ids)\n",
    "\n",
    "cache_path = outdir+outfile+'.pkl'\n",
    "check_path(cache_path)\n",
    "\n",
    "with open(cache_path, 'wb') as fout:\n",
    "    pickle.dump((all_input_ids, all_input_mask, all_segment_ids, all_sub_span, all_rel_span, all_obj_span, entity_ids), fout)\n",
    "print('Inputs dumped')\n",
    "\n",
    "# should be number of rows in umls.csv (1212586)\n",
    "print(loops + repeats + len(all_input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8dc362a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 1024, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 1024)\n",
       "    (token_type_embeddings): Embedding(2, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache_path = outdir+outfile+'.pkl'\n",
    "\n",
    "with open(cache_path, 'rb') as fin:\n",
    "    all_input_ids, all_input_mask, all_segment_ids, all_sub_span, all_rel_span, all_obj_span, entity_ids = pickle.load(fin)\n",
    "\n",
    "all_input_ids, all_input_mask, all_segment_ids, all_sub_span, all_rel_span, all_obj_span = [torch.tensor(x, dtype=torch.long) for x in [all_input_ids, all_input_mask, all_segment_ids, all_sub_span, all_rel_span, all_obj_span]]\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aab931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 18290/18290 [19:32:13<00:00,  3.85s/it]   \n"
     ]
    }
   ],
   "source": [
    "# setup loop variables\n",
    "n = entity_ids.shape[0]\n",
    "batch_size = 64\n",
    "\n",
    "assert n == all_input_ids.shape[0]\n",
    "\n",
    "# which layer of BERT to use for embeddings\n",
    "layer = -1\n",
    "emb_dim = 1024\n",
    "umls_concept_emb = torch.zeros((len(concept2id), emb_dim)).to(device)\n",
    "umls_rel_emb = torch.zeros((len(relation2id), emb_dim)).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mask = torch.arange(max_seq_length, device=device)[None, :]\n",
    "\n",
    "    for a in tqdm(range(0, n, batch_size), total=n // batch_size + 1, desc='Extracting features'):\n",
    "        b = min(a + batch_size, n)\n",
    "        *batch, sub_span, rel_span, obj_span = [x.to(device) for x in [all_input_ids[a:b], all_input_mask[a:b], all_segment_ids[a:b], all_sub_span[a:b], all_rel_span[a:b], all_obj_span[a:b]]]\n",
    "        outputs = model(*batch)\n",
    "        \n",
    "        hidden_states = outputs[-1][layer]\n",
    "        \n",
    "        sub_mask = (mask >= sub_span[:, 0, None]) & (mask <= sub_span[:, 1, None])\n",
    "        rel_mask = (mask >= rel_span[:, 0, None]) & (mask <= rel_span[:, 1, None])\n",
    "        obj_mask = (mask >= obj_span[:, 0, None]) & (mask <= obj_span[:, 1, None])\n",
    "        # apply mask using the spans, and average the token rep by dividing by span length\n",
    "        sub_pooled = (hidden_states * sub_mask.float().unsqueeze(-1)).sum(1) / (sub_span[:,1].float() - sub_span[:,0].float() + 1).unsqueeze(1)\n",
    "        rel_pooled = (hidden_states * rel_mask.float().unsqueeze(-1)).sum(1) / (rel_span[:,1].float() - rel_span[:,0].float() + 1).unsqueeze(1)\n",
    "        obj_pooled = (hidden_states * obj_mask.float().unsqueeze(-1)).sum(1) / (obj_span[:,1].float() - obj_span[:,0].float() + 1).unsqueeze(1)\n",
    "        \n",
    "        sub_ids = entity_ids[a:b, 0]\n",
    "        rel_ids = entity_ids[a:b, 1]\n",
    "        obj_ids = entity_ids[a:b, 2]\n",
    "        for i, (sub_id, rel_id, obj_id) in enumerate(zip(sub_ids, rel_ids, obj_ids)):\n",
    "            umls_concept_emb[concept2id[sub_id]] += sub_pooled[i]\n",
    "            umls_concept_emb[concept2id[obj_id]] += obj_pooled[i]\n",
    "            umls_rel_emb[int(rel_id)] += rel_pooled[i]\n",
    "\n",
    "output_dir = '../data/umls/encoder_embs/'\n",
    "check_path(output_dir)\n",
    "# np.save(output_dir + 'umls_concept_embs_not_counted', umls_concept_emb.to('cpu').numpy())\n",
    "# np.save(output_dir + 'umls_relation_embs_not_counted', umls_rel_emb.to('cpu').numpy())\n",
    "\n",
    "prune = False\n",
    "sub_unique, sub_counts = np.unique([concept2id[id] for id in entity_ids[:,0]], return_counts=True) \n",
    "rel_unique, rel_counts_unordered = np.unique(entity_ids[:,1].astype(int), return_counts=True) \n",
    "obj_unique, obj_counts = np.unique([concept2id[id] for id in entity_ids[:,2]], return_counts=True) \n",
    "rel_counts = np.zeros(len(id2relation))\n",
    "rel_counts[rel_unique] += rel_counts_unordered\n",
    "concept_counts = np.zeros(len(concept2id))\n",
    "concept_counts[sub_unique] += sub_counts\n",
    "concept_counts[obj_unique] += obj_counts \n",
    "\n",
    "umls_concept_emb = umls_concept_emb.to('cpu').numpy()\n",
    "umls_rel_emb = umls_rel_emb.to('cpu').numpy()\n",
    "umls_concept_emb = np.divide(umls_concept_emb, concept_counts[:,np.newaxis], out=np.zeros_like(umls_concept_emb), where=concept_counts[:,np.newaxis]!=0)\n",
    "umls_rel_emb = umls_rel_emb / rel_counts[:,np.newaxis]\n",
    "np.save(output_dir + 'bert-large_umls_concept_embs', umls_concept_emb)\n",
    "np.save(output_dir + 'bert-large_umls_relation_embs', umls_rel_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qaspa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
